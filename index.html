<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>Blending Data-Driven Priors in Dynamic Games </title>
<!--    <meta property="og:image" content="img/hri.webp" />-->
    <meta name="description" content="Blending Data-Driven Priors in Dynamic Games">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

<!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
  <!-- <link rel="icon" type="image/png" href="img/seal_icon.png"> -->

    <link href="//cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/styles/default.min.css">
    <link rel="stylesheet" href="css/app.css">
    <link rel="icon" href="img/hri_transparent.png">

    <script src="//cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/highlight.min.js"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
	
    <style>
        .nav-pills {
          position: relative;
          display: inline;
        }
        .imtip {
          position: absolute;
          top: 0;
          left: 0;
        }
        video {
          height: 100%;
          width: 100%;
          object-fit: cover;
        }
    </style>
</head>
<body>
    <div class="container" id="main">
        <div class="row mt-4">
            <h2 class="col-md-12 text-center">
                Blending Data-Driven Priors in Dynamic Games</br>
            </h2>
        </div>
        
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <br>
                    <li><a href="https://jlidard.github.io">Justin Lidard*</a></li>
                    <li><a href="https://haiminhu.org/">Haimin Hu*</a></li>
                    <li><a href="https://zzx9636.github.io/">Zixu Zhang</a></li>
                    <li><a href="https://aasherh.github.io/">Asher Hancock</a></li>
                    <li><a href="https://www.linkedin.com/in/albert-gim%C3%B3-contreras-a0894b25b?originalSubdomain=es">Albert Gimó Contreras</a></li>
                    <li><a href="https://www.linkedin.com/in/vikash-modi/">Vikash Modi</a></li>
                </ul>
            </div>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <br>
                    <li><a href="https://jadecastro.github.io/">Jonathan DeCastro</a></li>
                    <li><a href="https://scholar.google.com/citations?user=qqMpCwYAAAAJ&hl=en">Deepak Gopinath</a></li>
                    <li><a href="http://people.csail.mit.edu/rosman/">Guy Rosman</a></li>
                    <li><a href="https://naomi.princeton.edu/">Naomi Leonard</a></li>
                    <li><a href="https://mariasantos.me/">María Santos</a></li>
                    <li><a href="https://ece.princeton.edu/people/jaime-fernandez-fisac">Jaime Fernández Fisac</a></li>
                </ul>
            </div>
        </div>

        <div class="row justify-content-md-center">
<!--            <div class="col-md-2 text-center">-->
<!--                <a href="https://irom-lab.princeton.edu/">-->
<!--                    <image src="img/irom_lab.png" height="35px", width="150px" ></image>-->
<!--                </a>-->
<!--            </div>-->
            <div class="col-md-12 text-center">
                <a href="https://www.princeton.edu/">
                    <image src="img/PU1line.svg" height="55px"></image>
                </a>
            </div>
        </div>
        <div class="row mt-2">
            <h3 class="col-md-12 text-center">
                RSS 2024 </br> </br>
            </h3>
        </div>

        <div class="row justify-content-md-center">
            <div class="col-md-2 text-center">
                <a href="https://arxiv.org/pdf/2403.15959.pdf">
                    <image src="img/paper.png" height="110px" width="85px"></image>
                <h4><strong>Paper</strong></h4>
                </a>
            </div>
            <div class="col-md-2 text-center">
                <a href="https://github.com/irom-lab/risk_calibrated_interactive_planning">
                <image src="img/github.png"  height="110px"></image>
                <h4><strong>Code</strong></h4>
                </a>
            </div>
            <div class="col-md-2 text-center">
                <a href="https://drive.google.com/drive/folders/13qAaPCHEcx93BTcPwwXSITSm7LKsAohr?usp=sharing">
                <image src="img/database.png"  height="110px"></image>
                <h4><strong>
                    Dataset
                </strong></h4>
                </a>
            </div>
        </div>



        <div class="row justify-content-md-center">
            <div class="col-md-10 col-lg-8">
                <h3 class="mt-4 mb-2">
                    Abstract
                </h3>
                <p class="text-justify">
                    As intelligent robots like autonomous vehicles become
                    increasingly deployed in the presence of people, the extent to
                    which these systems should leverage model-based game-theoretic
                    planners versus data-driven policies for safe, interaction-aware
                    motion planning remains an open question. Existing dynamic
                    game formulations assume all agents are task-driven and behave
                    optimally. However, in reality, humans tend to deviate from the
                    decisions prescribed by these models, and their behavior is better
                    approximated under a noisy-rational paradigm. In this work,
                    we investigate a principled methodology to blend a data-driven
                    reference policy with an optimization-based game-theoretic policy.
                    We formulate KLGame, an algorithm for solving non-cooperative
                    dynamic game with Kullback-Leibler (KL) regularization with
                    respect to a general, stochastic, and possibly multi-modal reference
                    policy. Our method incorporates, for each decision maker, a
                    tunable parameter that permits modulation between task-driven
                    and data-driven behaviors. We propose an efficient algorithm for
                    computing multi-modal approximate feedback Nash equilibrium
                    strategies of KLGame in real time. Through a series of simulated
                    and real-world autonomous driving scenarios, we demonstrate
                    that KLGame policies can more effectively incorporate guidance
                    from the reference policy and account for noisily-rational human
                    behaviors versus non-regularized baselines
                </p>
                <div class="col-md-12">
                    <video id="v0" width="60%" preload="metadata" playsinline controls>
                        <source src="videos/rcip_compressed.mp4" type="video/mp4">
                    </video>
                </div>
                <p style="text-align:center;">
                    <image src="img/fig1.png" width="100%"></image>
<!--                    <embed type="application/pdf" src="img/figure1_tricolumn-compressed.pdf"></embed>-->
                </p>

            </div>
        </div>

        <div class="row justify-content-md-center">
            <div class="col-md-10 col-lg-8">
                <h3 class="mt-4 mb-2">
                    Contributions
                    <!-- Goal -->
                </h3>
                <p class="text-justify">
                    We introduce KLGame, a novel stochastic dynamic game
                    that blends interaction-aware task optimization with
                    closed-loop policy guidance via Kullback-Leibler (KL)
                    regularization. We provide an in-depth analysis in the
                    linear-quadratic (LQ) setting with Gaussian reference
                    policies and show KLGame permits an analytical global
                    feedback Nash equilibrium, which naturally generalizes
                    the solution of the maximum-entropy game.
                </p>
            </div>
        </div>

        <div class="row justify-content-md-center">
            <div class="col-md-10 col-lg-8">
                <h3 class="mt-4 mb-2">
                    Approach
                </h3>
                <p class="text-justify">
                    RCIP builds upon statistical risk calibration (SRC) to formally quantify and bound multiple notions of
                    risk in human-robot interaction (HRI).  Using a small set of calibration scenarios, RCIP computes step-wise prediction
                    losses to form an aggregate emperical risk estimate. Using a risk limit, for each pair (λ,θ) of prediction thresholds
                    and tunable model parameters, RCIP evaluates the hypothesis that the test set risk is above the limit.
                    Thus, for all hypotheses that are rejected, the test set risk satisfies the threshold (with high probability).
                </p>
                <p style="text-align:center;">
                    <image src="img/fig2.png" width="100%"></image>
<!--                    <embed type="application/pdf" src="img/figure1_tricolumn-compressed.pdf"></embed>-->
                </p>

                <p class="text-justify">
                    RCIP can be applied in multi-step settings using novel extensions to statistical risk calibration
                    derived in our work. RCIP provides a flexible framework for optimizing the episode-level task
                    completion and human intervention rates on both sequence and step levels. KnowNo, which generates plans in open-ended language, may generate a plan that is technically correct,
                    but ambiguous to execute for a language-conditioned policy (both the blue and white bin have a pot). RCIP instead guarantees
                    that the human’s intent is satisfied via constraint satisfaction with the intent-conditioned planner
                </p>
                                <p style="text-align:center;">
                    <image src="img/fig4.png" width="100%"></image>
<!--                    <embed type="application/pdf" src="img/figure1_tricolumn-compressed.pdf"></embed>-->
                </p>
                <p class="text-justify">
                    RCIP also imbues prediction models with task knowledge through a discrete set of human intents. An open-ended
                    VLM planner may generate a plan that is technically correct,
                    but ambiguous to execute for a language-conditioned policy. RCIP instead guarantees
                    that the human’s intent is satisfied via constraint satisfaction with the intent-conditioned planner.
                </p>

            </div>
        </div>

<!--        <div class="row justify-content-md-center">-->
<!--            <div class="col-md-10 col-lg-8">-->
<!--                <h3 class="mt-4 mb-2">-->
<!--                    TODO: bimanual videos-->
<!--                </h3>-->
<!--            </div>-->
<!--        </div>-->

        <div class="row justify-content-md-center">
            <div class="col-md-10 col-lg-8">
                <h3 class="mt-4 mb-2">
                    Citation
                </h3>
                <pre><code class="codeblock">
                    @article{lidard2024risk,
                      title={Risk-Calibrated Human-Robot Interaction via Set-Valued Intent Prediction},
                      author={Lidard, Justin and Pham, Hang and Bachman, Ariel and Boateng, Bryan and Majumdar, Anirudha},
                      journal={arXiv preprint arXiv:2403.15959},
                      year={2024}
                    }
                </code></pre>
            </div>
        </div>

    </div>
</body>
</html>
